"""
Content Validation System for Curio News

This module provides comprehensive validation for all content sections generated by the 6 specialized agents.
It ensures content quality, structure validation, and provides detailed validation reports for debugging.
"""

import json
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

class ValidationSeverity(Enum):
    """Severity levels for validation issues"""
    CRITICAL = "critical"  # Content cannot be used
    WARNING = "warning"   # Content has issues but can be used
    INFO = "info"        # Minor issues or suggestions

@dataclass
class ValidationIssue:
    """Represents a validation issue found in content"""
    field: str
    severity: ValidationSeverity
    message: str
    expected: Optional[str] = None
    actual: Optional[str] = None
    suggestion: Optional[str] = None

@dataclass
class ValidationResult:
    """Result of content validation"""
    is_valid: bool
    section: str
    issues: List[ValidationIssue]
    score: float  # 0-100 quality score
    metadata: Dict[str, Any]
    
    def has_critical_issues(self) -> bool:
        """Check if there are any critical validation issues"""
        return any(issue.severity == ValidationSeverity.CRITICAL for issue in self.issues)
    
    def get_issues_by_severity(self, severity: ValidationSeverity) -> List[ValidationIssue]:
        """Get issues filtered by severity level"""
        return [issue for issue in self.issues if issue.severity == severity]

class ContentValidator:
    """Comprehensive content validation system for all Curio News content sections"""
    
    def __init__(self):
        self.validation_rules = self._initialize_validation_rules()
        self.quality_thresholds = {
            'news_stories': {'min_score': 70, 'min_count': 3, 'max_count': 8},
            'favorite_story': {'min_score': 75, 'required_fields': ['reasoning']},
            'weekend_recommendations': {'min_score': 65, 'min_categories': 2},
            'visual_enhancements': {'min_score': 60, 'required_media': 1},
            'script_content': {'min_score': 80, 'min_words': 150, 'max_words': 300},
            'audio_metadata': {'min_score': 70, 'required_timings': True}
        }
    
    def validate_complete_content(self, content: Dict) -> Dict[str, ValidationResult]:
        """
        Validate all content sections and return comprehensive results
        
        Args:
            content: Complete content dictionary from agent orchestration
            
        Returns:
            Dictionary mapping section names to ValidationResult objects
        """
        validation_results = {}
        
        try:
            # Validate each content section
            sections_to_validate = [
                ('news_stories', content.get('news_items', [])),
                ('favorite_story', content.get('agentOutputs', {}).get('favoriteStory')),
                ('weekend_recommendations', content.get('agentOutputs', {}).get('weekendRecommendations')),
                ('visual_enhancements', content.get('agentOutputs', {}).get('mediaEnhancements')),
                ('script_content', content.get('script', '')),
                ('audio_metadata', {
                    'audio_url': content.get('audioUrl', ''),
                    'word_timings': content.get('word_timings', [])
                })
            ]
            
            for section_name, section_data in sections_to_validate:
                try:
                    result = self._validate_section(section_name, section_data)
                    validation_results[section_name] = result
                except Exception as e:
                    # Create error result for failed validation
                    validation_results[section_name] = ValidationResult(
                        is_valid=False,
                        section=section_name,
                        issues=[ValidationIssue(
                            field="validation_system",
                            severity=ValidationSeverity.CRITICAL,
                            message=f"Validation failed: {str(e)}"
                        )],
                        score=0.0,
                        metadata={'validation_error': str(e)}
                    )
            
            # Add overall validation summary
            validation_results['_summary'] = self._create_validation_summary(validation_results)
            
        except Exception as e:
            print(f"âŒ Critical error in content validation: {e}")
            validation_results['_error'] = ValidationResult(
                is_valid=False,
                section="system",
                issues=[ValidationIssue(
                    field="validation_system",
                    severity=ValidationSeverity.CRITICAL,
                    message=f"System validation error: {str(e)}"
                )],
                score=0.0,
                metadata={'system_error': str(e)}
            )
        
        return validation_results
    
    def _validate_section(self, section_name: str, data: Any) -> ValidationResult:
        """Validate a specific content section"""
        validator_method = getattr(self, f'validate_{section_name}', None)
        
        if not validator_method:
            return ValidationResult(
                is_valid=False,
                section=section_name,
                issues=[ValidationIssue(
                    field="validator",
                    severity=ValidationSeverity.CRITICAL,
                    message=f"No validator found for section: {section_name}"
                )],
                score=0.0,
                metadata={'missing_validator': True}
            )
        
        return validator_method(data)
    
    def validate_news_stories(self, stories: List[Dict]) -> ValidationResult:
        """Validate news stories content structure and quality"""
        issues = []
        score = 100.0
        metadata = {
            'story_count': len(stories) if stories else 0,
            'categories': [],
            'avg_relevance': 0.0,
            'validation_timestamp': datetime.utcnow().isoformat()
        }
        
        # Check if stories exist
        if not stories:
            issues.append(ValidationIssue(
                field="stories",
                severity=ValidationSeverity.CRITICAL,
                message="No news stories found",
                expected="At least 3 stories",
                actual="0 stories"
            ))
            score = 0.0
        else:
            # Validate story count
            thresholds = self.quality_thresholds['news_stories']
            if len(stories) < thresholds['min_count']:
                issues.append(ValidationIssue(
                    field="story_count",
                    severity=ValidationSeverity.CRITICAL,
                    message=f"Insufficient number of stories",
                    expected=f"At least {thresholds['min_count']} stories",
                    actual=f"{len(stories)} stories"
                ))
                score -= 30
            elif len(stories) > thresholds['max_count']:
                issues.append(ValidationIssue(
                    field="story_count",
                    severity=ValidationSeverity.WARNING,
                    message=f"Too many stories may overwhelm users",
                    expected=f"Maximum {thresholds['max_count']} stories",
                    actual=f"{len(stories)} stories"
                ))
                score -= 10
            
            # Validate individual stories
            categories = []
            relevance_scores = []
            
            for i, story in enumerate(stories):
                story_issues, story_score = self._validate_individual_story(story, i)
                issues.extend(story_issues)
                score = min(score, score * (story_score / 100))
                
                # Collect metadata
                if 'category' in story:
                    categories.append(story['category'])
                if 'relevance_score' in story:
                    relevance_scores.append(story['relevance_score'])
            
            # Check category diversity
            unique_categories = set(categories)
            if len(unique_categories) < 2:
                issues.append(ValidationIssue(
                    field="category_diversity",
                    severity=ValidationSeverity.WARNING,
                    message="Limited category diversity",
                    expected="At least 2 different categories",
                    actual=f"{len(unique_categories)} categories",
                    suggestion="Include stories from different news categories"
                ))
                score -= 15
            
            metadata['categories'] = list(unique_categories)
            metadata['avg_relevance'] = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
        
        return ValidationResult(
            is_valid=score >= self.quality_thresholds['news_stories']['min_score'],
            section="news_stories",
            issues=issues,
            score=max(0.0, score),
            metadata=metadata
        )
    
    def _validate_individual_story(self, story: Dict, index: int) -> Tuple[List[ValidationIssue], float]:
        """Validate an individual news story"""
        issues = []
        score = 100.0
        
        required_fields = ['title', 'summary', 'category']
        optional_fields = ['relevance_score', 'image', 'full_text', 'selection_reason']
        
        # Check required fields
        for field in required_fields:
            if field not in story or not story[field]:
                issues.append(ValidationIssue(
                    field=f"story[{index}].{field}",
                    severity=ValidationSeverity.CRITICAL,
                    message=f"Missing required field: {field}",
                    expected=f"Non-empty {field}",
                    actual="Missing or empty"
                ))
                score -= 25
            elif isinstance(story[field], str):
                # Validate string content
                if len(story[field].strip()) < 10:
                    issues.append(ValidationIssue(
                        field=f"story[{index}].{field}",
                        severity=ValidationSeverity.WARNING,
                        message=f"{field} is too short",
                        expected="At least 10 characters",
                        actual=f"{len(story[field])} characters"
                    ))
                    score -= 10
        
        # Validate title quality
        if 'title' in story and story['title']:
            title = story['title']
            if len(title) > 100:
                issues.append(ValidationIssue(
                    field=f"story[{index}].title",
                    severity=ValidationSeverity.WARNING,
                    message="Title is too long",
                    expected="Maximum 100 characters",
                    actual=f"{len(title)} characters"
                ))
                score -= 5
            
            # Check for clickbait patterns
            clickbait_patterns = [
                r'you won\'t believe',
                r'shocking',
                r'this will blow your mind',
                r'doctors hate',
                r'one weird trick'
            ]
            
            for pattern in clickbait_patterns:
                if re.search(pattern, title, re.IGNORECASE):
                    issues.append(ValidationIssue(
                        field=f"story[{index}].title",
                        severity=ValidationSeverity.WARNING,
                        message="Title may contain clickbait language",
                        suggestion="Use more neutral, informative language"
                    ))
                    score -= 5
                    break
        
        # Validate relevance score
        if 'relevance_score' in story:
            relevance = story['relevance_score']
            if not isinstance(relevance, (int, float)) or relevance < 0 or relevance > 1:
                issues.append(ValidationIssue(
                    field=f"story[{index}].relevance_score",
                    severity=ValidationSeverity.WARNING,
                    message="Invalid relevance score",
                    expected="Number between 0 and 1",
                    actual=str(relevance)
                ))
                score -= 10
        
        # Validate category
        if 'category' in story and story['category']:
            valid_categories = [
                'TECHNOLOGY', 'POLITICS', 'BUSINESS', 'SCIENCE', 
                'CULTURE', 'INTERNATIONAL', 'SPORTS', 'HEALTH'
            ]
            if story['category'].upper() not in valid_categories:
                issues.append(ValidationIssue(
                    field=f"story[{index}].category",
                    severity=ValidationSeverity.INFO,
                    message="Unusual category",
                    expected=f"One of: {', '.join(valid_categories)}",
                    actual=story['category']
                ))
        
        return issues, score
    
    def validate_favorite_story(self, favorite_story: Dict) -> ValidationResult:
        """Validate favorite story selection and reasoning"""
        issues = []
        score = 100.0
        metadata = {
            'has_reasoning': False,
            'reasoning_length': 0,
            'validation_timestamp': datetime.utcnow().isoformat()
        }
        
        if not favorite_story:
            issues.append(ValidationIssue(
                field="favorite_story",
                severity=ValidationSeverity.CRITICAL,
                message="No favorite story found",
                expected="Favorite story object with reasoning",
                actual="None"
            ))
            score = 0.0
        else:
            # Check for reasoning
            if 'reasoning' not in favorite_story or not favorite_story['reasoning']:
                issues.append(ValidationIssue(
                    field="favorite_story.reasoning",
                    severity=ValidationSeverity.CRITICAL,
                    message="Missing reasoning for favorite story selection",
                    expected="Explanation of why this story was selected",
                    actual="Missing or empty"
                ))
                score -= 40
            else:
                reasoning = favorite_story['reasoning']
                metadata['has_reasoning'] = True
                metadata['reasoning_length'] = len(reasoning)
                
                # Validate reasoning quality
                if len(reasoning) < 50:
                    issues.append(ValidationIssue(
                        field="favorite_story.reasoning",
                        severity=ValidationSeverity.WARNING,
                        message="Reasoning is too brief",
                        expected="At least 50 characters of explanation",
                        actual=f"{len(reasoning)} characters"
                    ))
                    score -= 20
                
                # Check for engagement indicators
                engagement_keywords = [
                    'fascinating', 'interesting', 'wow', 'amazing', 'breakthrough',
                    'surprising', 'curious', 'remarkable', 'significant', 'impact'
                ]
                
                has_engagement = any(keyword in reasoning.lower() for keyword in engagement_keywords)
                if not has_engagement:
                    issues.append(ValidationIssue(
                        field="favorite_story.reasoning",
                        severity=ValidationSeverity.INFO,
                        message="Reasoning lacks engagement indicators",
                        suggestion="Include words that convey excitement or curiosity"
                    ))
                    score -= 10
        
        return ValidationResult(
            is_valid=score >= self.quality_thresholds['favorite_story']['min_score'],
            section="favorite_story",
            issues=issues,
            score=max(0.0, score),
            metadata=metadata
        )
    
    def validate_weekend_recommendations(self, recommendations: Dict) -> ValidationResult:
        """Validate weekend recommendations content"""
        issues = []
        score = 100.0
        metadata = {
            'categories_found': [],
            'total_recommendations': 0,
            'validation_timestamp': datetime.utcnow().isoformat()
        }
        
        if not recommendations:
            issues.append(ValidationIssue(
                field="weekend_recommendations",
                severity=ValidationSeverity.CRITICAL,
                message="No weekend recommendations found",
                expected="Recommendations object with categories",
                actual="None"
            ))
            score = 0.0
        else:
            expected_categories = ['books', 'movies_and_shows', 'events']
            found_categories = []
            total_items = 0
            
            for category in expected_categories:
                if category in recommendations and recommendations[category]:
                    found_categories.append(category)
                    category_items = recommendations[category]
                    
                    if isinstance(category_items, list):
                        total_items += len(category_items)
                        
                        # Validate individual items in category
                        for i, item in enumerate(category_items):
                            item_issues = self._validate_recommendation_item(item, category, i)
                            issues.extend(item_issues)
                    else:
                        issues.append(ValidationIssue(
                            field=f"weekend_recommendations.{category}",
                            severity=ValidationSeverity.WARNING,
                            message=f"{category} should be a list",
                            expected="List of recommendation items",
                            actual=type(category_items).__name__
                        ))
                        score -= 10
                else:
                    issues.append(ValidationIssue(
                        field=f"weekend_recommendations.{category}",
                        severity=ValidationSeverity.WARNING,
                        message=f"Missing {category} recommendations",
                        expected=f"List of {category} recommendations",
                        actual="Missing"
                    ))
                    score -= 15
            
            metadata['categories_found'] = found_categories
            metadata['total_recommendations'] = total_items
            
            # Check minimum categories
            min_categories = self.quality_thresholds['weekend_recommendations']['min_categories']
            if len(found_categories) < min_categories:
                issues.append(ValidationIssue(
                    field="weekend_recommendations.categories",
                    severity=ValidationSeverity.WARNING,
                    message="Insufficient recommendation categories",
                    expected=f"At least {min_categories} categories",
                    actual=f"{len(found_categories)} categories"
                ))
                score -= 20
            
            # Check for cultural insights
            if 'cultural_insights' in recommendations:
                insights = recommendations['cultural_insights']
                if not isinstance(insights, dict) or not insights:
                    issues.append(ValidationIssue(
                        field="weekend_recommendations.cultural_insights",
                        severity=ValidationSeverity.INFO,
                        message="Cultural insights missing or invalid",
                        suggestion="Include cultural context for recommendations"
                    ))
                    score -= 5
        
        return ValidationResult(
            is_valid=score >= self.quality_thresholds['weekend_recommendations']['min_score'],
            section="weekend_recommendations",
            issues=issues,
            score=max(0.0, score),
            metadata=metadata
        )
    
    def _validate_recommendation_item(self, item: Dict, category: str, index: int) -> List[ValidationIssue]:
        """Validate individual recommendation item"""
        issues = []
        
        required_fields = {
            'books': ['title', 'author', 'description'],
            'movies_and_shows': ['title', 'platform', 'description'],
            'events': ['name', 'location', 'description']
        }
        
        category_fields = required_fields.get(category, ['title', 'description'])
        
        for field in category_fields:
            if field not in item or not item[field]:
                issues.append(ValidationIssue(
                    field=f"weekend_recommendations.{category}[{index}].{field}",
                    severity=ValidationSeverity.WARNING,
                    message=f"Missing {field} in {category} recommendation",
                    expected=f"Non-empty {field}",
                    actual="Missing or empty"
                ))
        
        return issues
    
    def validate_visual_enhancements(self, enhancements: Dict) -> ValidationResult:
        """Validate visual enhancements and media content"""
        issues = []
        score = 100.0
        metadata = {
            'has_stories': False,
            'story_count': 0,
            'media_types': [],
            'validation_timestamp': datetime.utcnow().isoformat()
        }
        
        if not enhancements:
            issues.append(ValidationIssue(
                field="visual_enhancements",
                severity=ValidationSeverity.WARNING,
                message="No visual enhancements found",
                expected="Media enhancements object",
                actual="None"
            ))
            score = 50.0  # Not critical, but reduces quality
        else:
            # Check for stories with media
            if 'stories' in enhancements and enhancements['stories']:
                stories = enhancements['stories']
                metadata['has_stories'] = True
                metadata['story_count'] = len(stories)
                
                for i, story in enumerate(stories):
                    story_issues = self._validate_media_story(story, i)
                    issues.extend(story_issues)
            else:
                issues.append(ValidationIssue(
                    field="visual_enhancements.stories",
                    severity=ValidationSeverity.WARNING,
                    message="No enhanced stories found",
                    expected="List of stories with media enhancements",
                    actual="Missing or empty"
                ))
                score -= 30
            
            # Check for description or other enhancement info
            if 'description' in enhancements and enhancements['description']:
                desc_length = len(enhancements['description'])
                if desc_length < 20:
                    issues.append(ValidationIssue(
                        field="visual_enhancements.description",
                        severity=ValidationSeverity.INFO,
                        message="Enhancement description is brief",
                        suggestion="Provide more detailed enhancement information"
                    ))
        
        return ValidationResult(
            is_valid=score >= self.quality_thresholds['visual_enhancements']['min_score'],
            section="visual_enhancements",
            issues=issues,
            score=max(0.0, score),
            metadata=metadata
        )
    
    def _validate_media_story(self, story: Dict, index: int) -> List[ValidationIssue]:
        """Validate individual media-enhanced story"""
        issues = []
        
        if 'title' not in story or not story['title']:
            issues.append(ValidationIssue(
                field=f"visual_enhancements.stories[{index}].title",
                severity=ValidationSeverity.WARNING,
                message="Media story missing title",
                expected="Story title",
                actual="Missing"
            ))
        
        if 'media_recommendations' in story:
            media = story['media_recommendations']
            
            # Check for images
            if 'images' in media and media['images']:
                for img_idx, img in enumerate(media['images']):
                    if 'url' not in img or not img['url']:
                        issues.append(ValidationIssue(
                            field=f"visual_enhancements.stories[{index}].media_recommendations.images[{img_idx}].url",
                            severity=ValidationSeverity.WARNING,
                            message="Image missing URL",
                            expected="Valid image URL",
                            actual="Missing"
                        ))
                    elif not img['url'].startswith(('http://', 'https://')):
                        issues.append(ValidationIssue(
                            field=f"visual_enhancements.stories[{index}].media_recommendations.images[{img_idx}].url",
                            severity=ValidationSeverity.WARNING,
                            message="Invalid image URL format",
                            expected="URL starting with http:// or https://",
                            actual=img['url'][:50] + "..." if len(img['url']) > 50 else img['url']
                        ))
        
        return issues
    
    def validate_script_content(self, script: str) -> ValidationResult:
        """Validate script content quality and structure"""
        issues = []
        score = 100.0
        metadata = {
            'word_count': 0,
            'sentence_count': 0,
            'has_millennial_tone': False,
            'readability_score': 0,
            'validation_timestamp': datetime.utcnow().isoformat()
        }
        
        if not script:
            issues.append(ValidationIssue(
                field="script_content",
                severity=ValidationSeverity.CRITICAL,
                message="No script content found",
                expected="Script text for audio generation",
                actual="Empty or missing"
            ))
            score = 0.0
        else:
            # Basic metrics
            words = script.split()
            sentences = re.split(r'[.!?]+', script)
            
            metadata['word_count'] = len(words)
            metadata['sentence_count'] = len([s for s in sentences if s.strip()])
            
            # Validate word count
            thresholds = self.quality_thresholds['script_content']
            if len(words) < thresholds['min_words']:
                issues.append(ValidationIssue(
                    field="script_content.length",
                    severity=ValidationSeverity.CRITICAL,
                    message="Script is too short",
                    expected=f"At least {thresholds['min_words']} words",
                    actual=f"{len(words)} words"
                ))
                score -= 30
            elif len(words) > thresholds['max_words']:
                issues.append(ValidationIssue(
                    field="script_content.length",
                    severity=ValidationSeverity.WARNING,
                    message="Script may be too long for target duration",
                    expected=f"Maximum {thresholds['max_words']} words",
                    actual=f"{len(words)} words"
                ))
                score -= 15
            
            # Check for millennial/Gen Z tone
            millennial_indicators = [
                'honestly', 'lowkey', 'ngl', 'get this', 'wild', 'plot twist',
                'okay so', 'here\'s the thing', 'basically', 'literally'
            ]
            
            script_lower = script.lower()
            found_indicators = [indicator for indicator in millennial_indicators if indicator in script_lower]
            
            if found_indicators:
                metadata['has_millennial_tone'] = True
            else:
                issues.append(ValidationIssue(
                    field="script_content.tone",
                    severity=ValidationSeverity.INFO,
                    message="Script may lack millennial/Gen Z tone",
                    suggestion="Consider adding conversational phrases like 'honestly', 'get this', or 'wild'"
                ))
                score -= 10
            
            # Check for contractions (conversational tone)
            contractions = ["don't", "won't", "can't", "isn't", "aren't", "wasn't", "weren't", "it's", "that's"]
            has_contractions = any(contraction in script_lower for contraction in contractions)
            
            if not has_contractions:
                issues.append(ValidationIssue(
                    field="script_content.conversational_tone",
                    severity=ValidationSeverity.INFO,
                    message="Script may sound too formal",
                    suggestion="Use contractions to make the tone more conversational"
                ))
                score -= 5
            
            # Check for excessive formal language
            formal_words = ['furthermore', 'moreover', 'consequently', 'nevertheless', 'henceforth']
            formal_found = [word for word in formal_words if word in script_lower]
            
            if formal_found:
                issues.append(ValidationIssue(
                    field="script_content.formality",
                    severity=ValidationSeverity.WARNING,
                    message="Script contains overly formal language",
                    actual=f"Found: {', '.join(formal_found)}",
                    suggestion="Use more casual, conversational language"
                ))
                score -= 15
            
            # Basic readability check (sentence length)
            if sentences:
                avg_sentence_length = len(words) / len([s for s in sentences if s.strip()])
                if avg_sentence_length > 20:
                    issues.append(ValidationIssue(
                        field="script_content.readability",
                        severity=ValidationSeverity.INFO,
                        message="Sentences may be too long for audio",
                        expected="Average 15-20 words per sentence",
                        actual=f"Average {avg_sentence_length:.1f} words per sentence",
                        suggestion="Break up long sentences for better audio flow"
                    ))
                    score -= 5
                
                metadata['readability_score'] = max(0, 100 - (avg_sentence_length - 15) * 2)
        
        return ValidationResult(
            is_valid=score >= self.quality_thresholds['script_content']['min_score'],
            section="script_content",
            issues=issues,
            score=max(0.0, score),
            metadata=metadata
        )
    
    def validate_audio_metadata(self, audio_data: Dict) -> ValidationResult:
        """Validate audio URL and word timing metadata"""
        issues = []
        score = 100.0
        metadata = {
            'has_audio_url': False,
            'has_word_timings': False,
            'timing_count': 0,
            'validation_timestamp': datetime.utcnow().isoformat()
        }
        
        # Validate audio URL
        audio_url = audio_data.get('audio_url', '')
        if not audio_url:
            issues.append(ValidationIssue(
                field="audio_metadata.audio_url",
                severity=ValidationSeverity.CRITICAL,
                message="No audio URL found",
                expected="Valid audio file URL",
                actual="Missing"
            ))
            score -= 40
        else:
            metadata['has_audio_url'] = True
            
            if not audio_url.startswith(('http://', 'https://')):
                issues.append(ValidationIssue(
                    field="audio_metadata.audio_url",
                    severity=ValidationSeverity.WARNING,
                    message="Audio URL format may be invalid",
                    expected="URL starting with http:// or https://",
                    actual=audio_url[:50] + "..." if len(audio_url) > 50 else audio_url
                ))
                score -= 15
            
            # Check for audio file extension
            audio_extensions = ['.mp3', '.wav', '.m4a', '.ogg']
            if not any(ext in audio_url.lower() for ext in audio_extensions):
                issues.append(ValidationIssue(
                    field="audio_metadata.audio_url",
                    severity=ValidationSeverity.INFO,
                    message="Audio URL doesn't contain recognizable audio extension",
                    suggestion="Ensure URL points to a valid audio file"
                ))
                score -= 5
        
        # Validate word timings
        word_timings = audio_data.get('word_timings', [])
        if not word_timings:
            issues.append(ValidationIssue(
                field="audio_metadata.word_timings",
                severity=ValidationSeverity.WARNING,
                message="No word timings found",
                expected="Array of word timing objects",
                actual="Missing or empty"
            ))
            score -= 25
        else:
            metadata['has_word_timings'] = True
            metadata['timing_count'] = len(word_timings)
            
            # Validate timing structure
            for i, timing in enumerate(word_timings[:5]):  # Check first 5 for performance
                if not isinstance(timing, dict):
                    issues.append(ValidationIssue(
                        field=f"audio_metadata.word_timings[{i}]",
                        severity=ValidationSeverity.WARNING,
                        message="Invalid word timing structure",
                        expected="Object with word, start, end properties",
                        actual=type(timing).__name__
                    ))
                    score -= 5
                    continue
                
                required_timing_fields = ['word', 'start', 'end']
                for field in required_timing_fields:
                    if field not in timing:
                        issues.append(ValidationIssue(
                            field=f"audio_metadata.word_timings[{i}].{field}",
                            severity=ValidationSeverity.WARNING,
                            message=f"Missing {field} in word timing",
                            expected=f"{field} property",
                            actual="Missing"
                        ))
                        score -= 3
        
        return ValidationResult(
            is_valid=score >= self.quality_thresholds['audio_metadata']['min_score'],
            section="audio_metadata",
            issues=issues,
            score=max(0.0, score),
            metadata=metadata
        )
    
    def _create_validation_summary(self, results: Dict[str, ValidationResult]) -> ValidationResult:
        """Create overall validation summary"""
        total_score = 0.0
        total_issues = []
        critical_sections = []
        warning_sections = []
        valid_sections = []
        
        section_count = 0
        for section_name, result in results.items():
            if section_name.startswith('_'):  # Skip summary sections
                continue
                
            section_count += 1
            total_score += result.score
            total_issues.extend(result.issues)
            
            if result.has_critical_issues():
                critical_sections.append(section_name)
            elif result.get_issues_by_severity(ValidationSeverity.WARNING):
                warning_sections.append(section_name)
            else:
                valid_sections.append(section_name)
        
        avg_score = total_score / section_count if section_count > 0 else 0.0
        overall_valid = len(critical_sections) == 0 and avg_score >= 70.0
        
        summary_metadata = {
            'total_sections': section_count,
            'average_score': round(avg_score, 1),
            'critical_sections': critical_sections,
            'warning_sections': warning_sections,
            'valid_sections': valid_sections,
            'total_issues': len(total_issues),
            'critical_issues': len([i for i in total_issues if i.severity == ValidationSeverity.CRITICAL]),
            'warning_issues': len([i for i in total_issues if i.severity == ValidationSeverity.WARNING]),
            'info_issues': len([i for i in total_issues if i.severity == ValidationSeverity.INFO]),
            'validation_timestamp': datetime.utcnow().isoformat()
        }
        
        summary_issues = []
        if critical_sections:
            summary_issues.append(ValidationIssue(
                field="overall_validation",
                severity=ValidationSeverity.CRITICAL,
                message=f"Critical issues in sections: {', '.join(critical_sections)}",
                suggestion="Fix critical issues before using content"
            ))
        
        if avg_score < 70.0:
            summary_issues.append(ValidationIssue(
                field="overall_quality",
                severity=ValidationSeverity.WARNING,
                message=f"Overall quality score below threshold",
                expected="Average score >= 70.0",
                actual=f"Average score: {avg_score:.1f}"
            ))
        
        return ValidationResult(
            is_valid=overall_valid,
            section="overall_summary",
            issues=summary_issues,
            score=avg_score,
            metadata=summary_metadata
        )
    
    def _initialize_validation_rules(self) -> Dict:
        """Initialize validation rules configuration"""
        return {
            'content_quality': {
                'min_text_length': 10,
                'max_title_length': 100,
                'required_story_fields': ['title', 'summary', 'category'],
                'valid_categories': [
                    'TECHNOLOGY', 'POLITICS', 'BUSINESS', 'SCIENCE',
                    'CULTURE', 'INTERNATIONAL', 'SPORTS', 'HEALTH'
                ]
            },
            'tone_validation': {
                'millennial_indicators': [
                    'honestly', 'lowkey', 'ngl', 'get this', 'wild', 'plot twist',
                    'okay so', 'here\'s the thing', 'basically', 'literally'
                ],
                'formal_words_to_avoid': [
                    'furthermore', 'moreover', 'consequently', 'nevertheless', 'henceforth'
                ],
                'contractions_expected': True
            },
            'media_validation': {
                'valid_url_schemes': ['http://', 'https://'],
                'audio_extensions': ['.mp3', '.wav', '.m4a', '.ogg'],
                'image_extensions': ['.jpg', '.jpeg', '.png', '.gif', '.webp']
            }
        }
    
    def generate_validation_report(self, validation_results: Dict[str, ValidationResult]) -> Dict:
        """Generate a comprehensive validation report for debugging"""
        report = {
            'report_timestamp': datetime.utcnow().isoformat(),
            'overall_status': 'UNKNOWN',
            'summary': {},
            'sections': {},
            'recommendations': [],
            'debug_info': {}
        }
        
        try:
            if '_summary' in validation_results:
                summary = validation_results['_summary']
                report['overall_status'] = 'VALID' if summary.is_valid else 'INVALID'
                report['summary'] = summary.metadata
            
            # Process each section
            for section_name, result in validation_results.items():
                if section_name.startswith('_'):
                    continue
                
                section_report = {
                    'is_valid': result.is_valid,
                    'score': result.score,
                    'issue_count': len(result.issues),
                    'critical_issues': [
                        {
                            'field': issue.field,
                            'message': issue.message,
                            'expected': issue.expected,
                            'actual': issue.actual,
                            'suggestion': issue.suggestion
                        }
                        for issue in result.get_issues_by_severity(ValidationSeverity.CRITICAL)
                    ],
                    'warnings': [
                        {
                            'field': issue.field,
                            'message': issue.message,
                            'suggestion': issue.suggestion
                        }
                        for issue in result.get_issues_by_severity(ValidationSeverity.WARNING)
                    ],
                    'metadata': result.metadata
                }
                
                report['sections'][section_name] = section_report
            
            # Generate recommendations
            report['recommendations'] = self._generate_recommendations(validation_results)
            
        except Exception as e:
            report['debug_info']['report_generation_error'] = str(e)
        
        return report
    
    def _generate_recommendations(self, validation_results: Dict[str, ValidationResult]) -> List[str]:
        """Generate actionable recommendations based on validation results"""
        recommendations = []
        
        for section_name, result in validation_results.items():
            if section_name.startswith('_'):
                continue
            
            if result.has_critical_issues():
                recommendations.append(f"ðŸš¨ Fix critical issues in {section_name} before using content")
            
            if result.score < 70:
                recommendations.append(f"âš ï¸ Improve quality of {section_name} (current score: {result.score:.1f})")
            
            # Add specific suggestions from issues
            for issue in result.issues:
                if issue.suggestion and issue.severity in [ValidationSeverity.CRITICAL, ValidationSeverity.WARNING]:
                    recommendations.append(f"ðŸ’¡ {section_name}: {issue.suggestion}")
        
        return recommendations[:10]  # Limit to top 10 recommendations